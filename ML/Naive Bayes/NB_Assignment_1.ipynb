{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ee67b69",
   "metadata": {},
   "source": [
    "### Q1. What is Bayes' theorem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57da0b4",
   "metadata": {},
   "source": [
    "* Bayes' theorem is a way of updating our beliefs about the likelihood of something happening, based on new information that we receive.\n",
    "<br>\n",
    "\n",
    "* For example, let's say that you want to know the likelihood of catching a cold. You know that the probability of catching a cold is generally low, but it increases if you are exposed to someone who already has a cold.\n",
    "<br>\n",
    "\n",
    "* Now, suppose you learn that your co-worker has just come down with a cold. This new information changes your initial belief about the likelihood of catching a cold. Bayes' theorem helps you to update your belief by taking into account the new information.\n",
    "<br>\n",
    "\n",
    "* In essence, Bayes' theorem is a mathematical formula that allows you to calculate the probability of something happening, given what you know about the situation. It's a tool that can help you make better decisions by incorporating new information and updating your beliefs accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7024ce5",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459985b1",
   "metadata": {},
   "source": [
    "### Q2. What is the formula for Bayes' theorem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926e2173",
   "metadata": {},
   "source": [
    "* The formula for Bayes' theorem is:\n",
    "> **P(H|E) = P(E|H) * P(H) / P(E)**\n",
    "\n",
    "* Where:\n",
    "> * P(H|E) is the posterior probability of H given E (what we want to know)\n",
    "> * P(E|H) is the likelihood of observing the evidence E given the hypothesis H (how well the evidence supports the hypothesis)\n",
    "> * P(H) is the prior probability of the hypothesis H (our initial belief in the hypothesis before seeing the evidence)\n",
    "> * P(E) is the marginal probability of the evidence E (the total probability of observing the evidence, regardless of the hypothesis)\n",
    "\n",
    "* This formula is used to update our beliefs about the probability of a hypothesis H being true based on new evidence E. It is a powerful tool in Bayesian statistics and machine learning, allowing us to make more accurate predictions and decisions based on data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcc82a5",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905f3b8a",
   "metadata": {},
   "source": [
    "### Q3. How is Bayes' theorem used in practice?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0376f5",
   "metadata": {},
   "source": [
    "* Bayes' theorem is used in a wide range of practical applications, from medical diagnosis to spam filtering. \n",
    "* Here are a few examples:\n",
    "    * **Medical diagnosis:** Bayes' theorem can be used to calculate the probability of a patient having a particular disease, based on their symptoms and other diagnostic tests. For example, if a patient presents with a certain set of symptoms, a doctor can use Bayes' theorem to calculate the probability of a specific disease, and then order additional tests to confirm the diagnosis.\n",
    "\n",
    "    * **Spam filtering:** Bayes' theorem can be used to classify emails as spam or non-spam. The algorithm looks at the words in the email and calculates the probability that the email is spam, based on the frequency of words that are commonly found in spam emails.\n",
    "\n",
    "    * **Risk assessment:** Bayes' theorem can be used to assess the risk of a particular event occurring, such as a natural disaster or a terrorist attack. The probability of the event can be estimated based on historical data and other relevant information, and then used to inform decision-making and risk management strategies.\n",
    "\n",
    "    * **Machine learning:** Bayes' theorem is used in various machine learning algorithms, such as Naive Bayes classifiers, which can be trained to predict the probability of a certain outcome based on a set of input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219a306d",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1b2a05",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between Bayes' theorem and conditional probability?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fcd8d5",
   "metadata": {},
   "source": [
    "* Bayes' theorem is essentially a statement about conditional probability. It provides a way to calculate the conditional probability of an event A, given some new information or evidence B.\n",
    "<br>\n",
    "\n",
    "* The formula for Bayes' theorem involves two conditional probabilities: \n",
    "> **P(A|B) and P(B|A). P(A|B)**\n",
    "> * It represents the probability of A given that B has occurred, while P(B|A) represents the probability of B given that A has occurred.\n",
    "<br>\n",
    "\n",
    "* Bayes' theorem tells us how to calculate the probability of A given B, based on these two conditional probabilities and the prior probabilities of A and B. In other words, it tells us how to update our beliefs about the probability of A, based on the new information provided by B.\n",
    "<br>\n",
    "\n",
    "* So, in essence, Bayes' theorem is a tool for working with conditional probabilities, and is particularly useful when we need to update our beliefs in light of new evidence or information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d318d5f",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b35011f",
   "metadata": {},
   "source": [
    "### Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2086dc2",
   "metadata": {},
   "source": [
    "* The Naive Bayes classifier is a simple and effective machine learning algorithm that is used for a wide range of applications such as text classification, spam filtering, sentiment analysis, and more. There are three types of Naive Bayes classifiers: Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes. The choice of which Naive Bayes classifier to use for a given problem depends on the nature of the data and the problem at hand. \n",
    "<br>\n",
    "\n",
    "* Here are some guidelines to help you choose the appropriate type of Naive Bayes classifier:\n",
    "\n",
    "    * **Gaussian Naive Bayes:** This type of classifier is used when the features are continuous variables that follow a Gaussian distribution. It is often used for problems such as image classification and spam filtering, where the features are numeric values.\n",
    "    * **Multinomial Naive Bayes:** This type of classifier is used when the features are discrete variables such as word counts in text classification problems. It is often used in natural language processing applications, such as sentiment analysis and text classification.\n",
    "    * **Bernoulli Naive Bayes:** This type of classifier is similar to the Multinomial Naive Bayes classifier, but it is used for solving some text classification problems when the features are binary variables such as the presence or absence of a particular word in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bec8d2d",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b59b4cc",
   "metadata": {},
   "source": [
    "### Q6. Assignment:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8858de83",
   "metadata": {},
   "source": [
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. \n",
    "You want to use Naive Bayes to classify a new instance with features X1 = 3 and X2 = 4. \n",
    "The following table shows the frequency of each feature value for each class:\n",
    "\n",
    "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "A 3 3 4 4 3 3 3\n",
    "B 2 2 1 2 2 2 3\n",
    "\n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\n",
    "to belong to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b353f030",
   "metadata": {},
   "source": [
    "* To classify the new instance with features X1 = 3 and X2 = 4 using Naive Bayes, we need to calculate the posterior probabilities for each class, given these feature values. We can do this using Bayes' theorem:\n",
    "    > **P(A|X1=3,X2=4) = P(X1=3,X2=4|A) * P(A) / P(X1=3,X2=4)**\n",
    "\n",
    "    > **P(B|X1=3,X2=4) = P(X1=3,X2=4|B) * P(B) / P(X1=3,X2=4)**\n",
    "<br>\n",
    "\n",
    "* Since the prior probabilities for A and B are assumed to be equal, we can simplify this to:\n",
    "    > **P(A|X1=3,X2=4) = P(X1=3,X2=4|A) / P(X1=3,X2=4)**\n",
    "\n",
    "    > **P(B|X1=3,X2=4) = P(X1=3,X2=4|B) / P(X1=3,X2=4)**\n",
    "<br>\n",
    "\n",
    "* To calculate the probabilities, we need to use the Naive Bayes assumption that the features are conditionally independent, given the class. This allows us to factorize the joint probability distribution as follows:\n",
    "    > **P(X1=3,X2=4|A) = P(X1=3|A) * P(X2=4|A)**\n",
    "\n",
    "    > **P(X1=3,X2=4|B) = P(X1=3|B) * P(X2=4|B)**\n",
    "<br>\n",
    "\n",
    "* We can estimate these probabilities from the frequency table provided:\n",
    "    > **P(X1=3|A) = 4/10**\n",
    "\n",
    "    > **P(X1=3|B) = 1/7**\n",
    "\n",
    "    > **P(X2=4|A) = 3/10**\n",
    "\n",
    "    > **P(X2=4|B) = 1/7**\n",
    "<br>\n",
    "\n",
    "* To calculate the denominator, we need to use the law of total probability:\n",
    "    > **P(X1=3,X2=4) = P(X1=3,X2=4|A) * P(A) + P(X1=3,X2=4|B) * P(B)**\n",
    "<br>\n",
    "\n",
    "* We can estimate these probabilities from the frequency table provided:\n",
    "\n",
    "    > **P(X1=3,X2=4|A) = P(X1=3|A) * P(X2=4|A) = (4/10) * (3/10) = 12/100**\n",
    "\n",
    "    > **P(X1=3,X2=4|B) = P(X1=3|B) * P(X2=4|B) = (1/7) * (1/7) = 1/49**\n",
    "\n",
    "    > **P(A) = P(B) = 0.5**\n",
    "<br>\n",
    "\n",
    "* Therefore:\n",
    "    > **P(X1=3,X2=4) = (12/100) * 0.5 + (1/49) * 0.5 = 0.124**\n",
    "<br>\n",
    "\n",
    "* Now we can plug these values into the formula for the posterior probabilities:\n",
    "    > **P(A|X1=3,X2=4) = (4/10) * (3/10) / 0.124 = 0.967**\n",
    "\n",
    "    > **P(B|X1=3,X2=4) = (1/7) * (1/7) / 0.124 = 0.033**\n",
    "<br>\n",
    "\n",
    "* Therefore, Naive Bayes would predict that the new instance with features X1=3 and X2=4 belongs to class A, since it has a much higher posterior probability than class B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6433991",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
