{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0457e354",
   "metadata": {},
   "source": [
    "### Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7ad2bf",
   "metadata": {},
   "source": [
    "* According to the information provided:\n",
    "    > P(A) = 70% = 0.70 (probability that an employee uses the health insurance plan)\n",
    "    \n",
    "    > P(B|A) = 40% = 0.40 (probability that an employee is a smoker given that they use the health insurance plan)\n",
    "<br>\n",
    "\n",
    "* Bayes' theorem states:\n",
    "    > P(B|A) = (P(A|B) * P(B)) / P(A)\n",
    "<br>\n",
    "\n",
    "* We know P(B|A) and P(A), but we need to find P(A|B) and P(B) to calculate the probability.\n",
    "<br>\n",
    "\n",
    "* Let's assume that the proportion of smokers in the overall employee population is S. In other words, P(B) represents the probability that a randomly selected employee is a smoker, regardless of their health insurance plan usage.\n",
    "<br>\n",
    "\n",
    "* Given that P(A|B) = 1 (if an employee is a smoker, they certainly use the health insurance plan), we can rewrite Bayes' theorem as:\n",
    "    > P(B|A) = (1 * P(B)) / P(A)\n",
    "<br>\n",
    "\n",
    "* Substituting the given values:\n",
    "    > 0.40 = (1 * P(B)) / 0.70\n",
    "<br>\n",
    "\n",
    "* Solving for P(B):\n",
    "    > 0.40 * 0.70 = P(B)\n",
    "    \n",
    "    > 0.28 = P(B)\n",
    "<br>\n",
    "\n",
    "* Therefore, the probability that an employee is a smoker given that they use the health insurance plan is 0.28 or 28%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1b55ea",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb4e619",
   "metadata": {},
   "source": [
    "### Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973eb712",
   "metadata": {},
   "source": [
    "* Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of the Naive Bayes algorithm, which is a probabilistic classification algorithm that is widely used in natural language processing, text mining, and other machine learning applications. Below are difference between Bernoullin and Multinomial Naive Bayes :\n",
    "<br>\n",
    "\n",
    "\n",
    "|Bernoulli Naive Bayes|Multinomial Naive Bayes|\n",
    "|--|--|\n",
    "|Assumes binary input data|Assumes count input data|\n",
    "|Represents each document as a binary vector|Represents each document as a count vector|\n",
    "|Calculates likelihood probabilities based on presence/absence of features|Calculates likelihood probabilities based on frequency of features|\n",
    "|Suitable for problems focused on the presence or absence of features|Suitable for problems focused on the frequency of features|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa7880b",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f4e8b4",
   "metadata": {},
   "source": [
    "### Q3. How does Bernoulli Naive Bayes handle missing values?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271d7313",
   "metadata": {},
   "source": [
    "* Bernoulli Naive Bayes is a classification algorithm that is commonly used in natural language processing tasks such as text classification. It is a variant of the Naive Bayes algorithm that assumes that the features are binary or Boolean, indicating whether a particular feature is present or not.\n",
    "\n",
    "<br>\n",
    "\n",
    "* In the case of missing values in the input data, Bernoulli Naive Bayes handles them by simply ignoring the missing values and treating them as if they were not present in the data. This is because the algorithm assumes that the features are independent of each other, and therefore the absence of a particular feature does not affect the probability of the presence of another feature.\n",
    "\n",
    "<br>\n",
    "\n",
    "* However, it is important to note that the presence or absence of certain features can have a significant impact on the classification accuracy of the algorithm. Therefore, it is recommended to handle missing values in the input data by imputing correct values, such as the mean or median value of that desired feature before applying the Bernoulli Naive Bayes algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22da3602",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45974036",
   "metadata": {},
   "source": [
    "### Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ddd1c3",
   "metadata": {},
   "source": [
    "* Yes, Gaussian Naive Bayes can be used for multi-class classification problems. In this case, the algorithm extends the binary Gaussian Naive Bayes classifier to the multi-class setting by using the \"one-vs-all\" (OvA) approach.\n",
    "<br>\n",
    "\n",
    "* In the OvA(\"one-vs-all\") approach, the multi-class problem is divided into multiple binary classification problems, with each class compared against all other classes. For example, if we have a problem with three classes (A, B, and C), we would train three binary classifiers: one to distinguish A from B and C, one to distinguish B from A and C, and one to distinguish C from A and B.\n",
    "<br>\n",
    "\n",
    "* During classification, the algorithm calculates the probability of each document belonging to each class using the corresponding binary classifier. The document is assigned to the class with the highest probability.\n",
    "<br>\n",
    "\n",
    "* In Gaussian Naive Bayes, the likelihood probability is modeled using a Gaussian distribution for each feature in each class. The algorithm estimates the mean and variance of each feature in each class based on the training data. During classification, the algorithm calculates the probability of each document belonging to each class using the Gaussian distribution parameters for that class.\n",
    "<br>\n",
    "\n",
    "* Overall, Gaussian Naive Bayes can be a useful algorithm for multi-class classification problems when the features are continuous and can be modeled using a Gaussian distribution. However, it is important to note that it makes certain assumptions about the data (such as independence of features) that may not always hold in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe35a427",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a918068",
   "metadata": {},
   "source": [
    "### Q5. Assignment:\n",
    "<br>\n",
    "\n",
    "* **Data preparation:**\n",
    "    * Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message is spam or not based on several input features.\n",
    "<br>\n",
    "\n",
    "* **Implementation:**\n",
    "    * Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the dataset. You should use the default hyperparameters for each classifier.\n",
    "<br>\n",
    "\n",
    "* **Results:**\n",
    "    * Report the following performance metrics for each classifier:\n",
    "        * Accuracy\n",
    "        * Precision\n",
    "        * Recall\n",
    "        * F1 score\n",
    "<br>    \n",
    "\n",
    "* **Discussion:**\n",
    "    *Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is the case? Are there any limitations of Naive Bayes that you observed?\n",
    "<br>\n",
    "\n",
    "* **Conclusion:**\n",
    "    * Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d94fc6b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('spambase.data',header=None)\n",
    "\n",
    "features=[]\n",
    "for i in range(df.shape[1]):\n",
    "    if i!=57:\n",
    "        fs = 'f'+str(i+1)\n",
    "        features.append(fs)\n",
    "    else:\n",
    "        features.append('target')\n",
    "df.columns = features\n",
    "\n",
    "# Seperating X and Y variables\n",
    "X = df.drop(labels=['target'],axis=1)\n",
    "Y = df[['target']]\n",
    "\n",
    "# Train Test Split \n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X,Y,test_size=0.3,random_state=42,stratify=Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b182bbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77564103 0.82191781 0.80267559 0.802589   0.78064516 0.81081081\n",
      " 0.82876712 0.82033898 0.80130293 0.8125    ]\n",
      "Results for Gaussian Naive Bayes\n",
      "Mean 10 fold cross validation f1 score is : 0.8057\n"
     ]
    }
   ],
   "source": [
    "## Gaussian NB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(xtrain,ytrain.values.flatten())\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf =  StratifiedKFold(n_splits=10,shuffle=True,random_state=42)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores_gnb = cross_val_score(GaussianNB(),xtrain,ytrain.values.flatten(),cv=skf,scoring='f1')\n",
    "print(scores_gnb)\n",
    "\n",
    "import numpy as np\n",
    "mean_score_gnb = np.mean(scores_gnb)\n",
    "print('Results for Gaussian Naive Bayes')\n",
    "print(f'Mean 10 fold cross validation f1 score is : {mean_score_gnb:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd8491b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84897959 0.84677419 0.84120172 0.8515625  0.85258964 0.81512605\n",
      " 0.8879668  0.85232068 0.85483871 0.84081633]\n",
      "Results for BernoulliNB :\n",
      "Mean 10 fold cross validation f1 score is : 0.8492\n"
     ]
    }
   ],
   "source": [
    "## Bernoulli NB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(xtrain,ytrain.values.flatten())\n",
    "\n",
    "scores_bnb = cross_val_score(BernoulliNB(),xtrain,ytrain.values.flatten(),cv=skf,scoring='f1')\n",
    "print(scores_bnb)\n",
    "\n",
    "mean_score_bnb = np.mean(scores_bnb)\n",
    "print('Results for BernoulliNB :')\n",
    "print(f'Mean 10 fold cross validation f1 score is : {mean_score_bnb:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f512f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70817121 0.68907563 0.74509804 0.71604938 0.67741935 0.72131148\n",
      " 0.76       0.712      0.703125   0.7768595 ]\n",
      "Results for MultinomialNB :\n",
      "Mean 10 fold cross validation f1 score is : 0.7209\n"
     ]
    }
   ],
   "source": [
    "## Multinomial Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(xtrain,ytrain.values.flatten())\n",
    "\n",
    "scores_mnb = cross_val_score(MultinomialNB(),xtrain,ytrain.values.flatten(),cv=skf,scoring='f1')\n",
    "print(scores_mnb)\n",
    "\n",
    "mean_score_mnb = np.mean(scores_mnb)\n",
    "print('Results for MultinomialNB :')\n",
    "print(f'Mean 10 fold cross validation f1 score is : {mean_score_mnb:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1756aa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to store all above metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "def evaluate_model(x,y,model):\n",
    "    ypred = model.predict(x)\n",
    "    acc = accuracy_score(y,ypred)\n",
    "    pre = precision_score(y,ypred)\n",
    "    rec = recall_score(y,ypred)\n",
    "    f1 = f1_score(y,ypred)\n",
    "    print(f'Accuracy  : {acc:.4f}')\n",
    "    print(f'Precision : {pre:.4f}')\n",
    "    print(f'Recall    : {rec:.4f}')\n",
    "    print(f'F1 Score  : {f1:.4f}')\n",
    "    return acc, pre, rec, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f39cd191",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes Results : \n",
      "Accuracy  : 0.8240\n",
      "Precision : 0.7048\n",
      "Recall    : 0.9522\n",
      "F1 Score  : 0.8100\n",
      "--------------------------------\n",
      "Bernoulli Naive Bayes Results : \n",
      "Accuracy  : 0.8870\n",
      "Precision : 0.8865\n",
      "Recall    : 0.8180\n",
      "F1 Score  : 0.8509\n",
      "--------------------------------\n",
      "Multinomial Naive Bayes Results : \n",
      "Accuracy  : 0.7697\n",
      "Precision : 0.7190\n",
      "Recall    : 0.6820\n",
      "F1 Score  : 0.7000\n"
     ]
    }
   ],
   "source": [
    "## Evaluate Gaussian NB\n",
    "print('Gaussian Naive Bayes Results : ')\n",
    "acc_gnb, pre_gnb, rec_gnb, f1_gnb = evaluate_model(xtest,ytest.values.flatten(),gnb)\n",
    "print('--------------------------------')\n",
    "## Evaluate Bernoulli NB\n",
    "print('Bernoulli Naive Bayes Results : ')\n",
    "acc_bnb, pre_bnb, rec_bnb, f1_bnb = evaluate_model(xtest,ytest.values.flatten(),bnb)\n",
    "print('--------------------------------')\n",
    "## Evaluate Multinomial NB\n",
    "print('Multinomial Naive Bayes Results : ')\n",
    "acc_mnb, pre_mnb, rec_mnb, f1_mnb = evaluate_model(xtest,ytest.values.flatten(),mnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29280328",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eccbd0",
   "metadata": {},
   "source": [
    "* **Discussion:**\n",
    "    * Best Model for above data is Bernoulli Naive Bayes\n",
    "\n",
    "        * **Reasons:**\n",
    "\n",
    "            * BernoulliNB has highest test f1 score of 0.8509\n",
    "            * BernoulliNB has highest test accuracy of 0.8870\n",
    "            * BernoulliNB has highest 10 fold cross validation F1 score of 0.8492\n",
    "            \n",
    "* Although Naive Bayes algorithm is a powerful and widely used algorithm, it also has some limitations, including:\n",
    "* The assumption of feature independence: The Naive Bayes algorithm assumes that the features are independent of each other. However, in real-world scenarios, this assumption is not always true, and features may be dependent on each other.\n",
    "\n",
    "* Sensitivity to input data: Naive Bayes algorithm is very sensitive to input data, and even a slight change in the input data can significantly affect the accuracy of the model.\n",
    "\n",
    "* Lack of tuning parameters: Naive Bayes algorithm does not have many tuning parameters that can be adjusted to improve its performance.\n",
    "\n",
    "* Data sparsity problem: Naive Bayes algorithm relies on a lot of training data to estimate the probabilities of different features. However, if some features have very low frequencies in the training data, the algorithm may not be able to accurately estimate their probabilities.\n",
    "\n",
    "* Class-conditional independence assumption: Naive Bayes algorithm assumes that each feature is conditionally independent given the class. However, in many cases, this assumption may not hold, and the algorithm may not perform well.\n",
    "\n",
    "* Imbalanced class distribution: Naive Bayes algorithm assumes that the classes are equally likely, but in real-world scenarios, the class distribution may be imbalanced, which can lead to biased results.\n",
    "\n",
    "* The need for continuous data: Naive Bayes algorithm assumes that the input features are continuous, which may not always be the case in real-world scenarios where the input features are discrete.\n",
    "\n",
    "<br>\n",
    "\n",
    "* **Conclusion:**\n",
    "    * Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "* **Below are conclusions for above model**\n",
    "    * Bernoulli Naive Bayes performed best on both cross validation and test dataset.\n",
    "    * For Email Classification Neural Network is better suited algorithm as it is able to provide better results and has lot of tunable paramenters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f47c55",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
