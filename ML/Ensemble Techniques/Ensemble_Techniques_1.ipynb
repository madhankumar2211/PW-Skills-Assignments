{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f13f56c",
   "metadata": {},
   "source": [
    "### Q1. What is an ensemble technique in machine learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747acfd8",
   "metadata": {},
   "source": [
    "* Ensemble techniques in machine learning involve combining multiple models or algorithms to improve the overall performance of a machine learning model. The basic idea behind ensemble techniques is to leverage the diversity of different models and to use their strengths to compensate for the weaknesses of other models. They're used for both classification and regression tasks. \n",
    "\n",
    "* There are two main types of ensemble techniques:\n",
    "<br>\n",
    "\n",
    "> * **Bagging:** This involves building multiple models using random subsets of the training data and combining their predictions. The most popular example of bagging is the random forest algorithm.\n",
    "> * **Boosting:** This involves building multiple models sequentially, where each subsequent model is trained to correct the errors made by the previous model. The most popular example of boosting is the AdaBoost algorithm.\n",
    "<br>\n",
    "\n",
    "* Ensemble techniques can also be combined with other machine learning techniques, such as neural networks or deep learning, to further improve performance. The key advantage of ensemble techniques is that they can reduce the risk of overfitting and improve the generalization ability of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78a86ac",
   "metadata": {},
   "source": [
    "# -------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762882f4",
   "metadata": {},
   "source": [
    "### Q2. Why are ensemble techniques used in machine learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf5e677",
   "metadata": {},
   "source": [
    "* Ensemble techniques are used in machine learning for several reasons:\n",
    "\n",
    "    * **Improved accuracy:** By combining multiple models, ensemble techniques can achieve higher accuracy than individual models. This is because each model may have its own strengths and weaknesses, and by combining them, the ensemble model can exploit the strengths of each individual model and mitigate the weaknesses.\n",
    "    * **Reduced overfitting:** Ensemble techniques can also reduce overfitting, which occurs when a model is too complex and learns to fit the training data too closely, resulting in poor performance on unseen data. By using multiple models, ensemble techniques can reduce the risk of overfitting and improve the generalization ability of the model.\n",
    "    * **Robustness:** Ensemble techniques can also improve the robustness of the model by reducing the impact of outliers or noisy data. Since different models may be sensitive to different types of noise, the ensemble model can be more robust by averaging out the predictions of multiple models.\n",
    "    * **Flexibility:** Ensemble techniques are flexible and it can be used with some data models, with respect to machine learing, that solves complex problems, such as, decision trees, neural networks, and support vector machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f335a5",
   "metadata": {},
   "source": [
    "# -------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e91bbf",
   "metadata": {},
   "source": [
    "### Q3. What is bagging?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3f41d",
   "metadata": {},
   "source": [
    "* A terminology that is widely used for ensemble techniques in machine learning, which involves building multiple models using random subsets of the training data and combining their predictions, is called bagging. The most popular example of bagging is the random forest algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c67dac",
   "metadata": {},
   "source": [
    "# -------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2292a86b",
   "metadata": {},
   "source": [
    "### Q4. What is boosting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91db183",
   "metadata": {},
   "source": [
    "* A terminology that is widely used for ensemble techniques in machine learning, which involves building multiple models sequentially, where each subsequent model is trained to correct the errors made by the previous model, is called boosting. The most popular example of boosting is the AdaBoost algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a851c4f5",
   "metadata": {},
   "source": [
    "# -------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42873443",
   "metadata": {},
   "source": [
    "### Q5. What are the benefits of using ensemble techniques?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b34075",
   "metadata": {},
   "source": [
    "* There are several benefits of using ensemble techniques in machine learning:\n",
    "\n",
    "    * **Improved Accuracy:** Ensemble techniques can improve the accuracy of a model by combining the predictions of multiple models. Since each model may have different strengths and weaknesses, the ensemble can leverage the strengths of each individual model and mitigate their weaknesses, resulting in higher accuracy.\n",
    "    * **Reduced Overfitting:** Ensemble techniques can reduce the risk of overfitting by using multiple models. Overfitting occurs when a model is too complex and learns to fit the training data too closely, which results in poor performance. By using multiple models, ensemble techniques can reduce the risk of overfitting and improve the generalization ability of the model.\n",
    "    * **Robustness:** Ensemble techniques can improve the robustness of a model by reducing the impact of outliers or noisy data. Since different models may be sensitive to different types of noise, the ensemble can be more robust by averaging out the predictions of multiple models.\n",
    "    * **Flexibility:** Ensemble techniques are flexible and it can be used with some data models, with respect to machine learing, that solves complex problems, such as, decision trees, neural networks, and support vector machines.\n",
    "    * **Faster Training Time:** In some cases, ensemble techniques can be faster to train than individual models. Also, it can parallelize the training of multiple models, allowing them to be trained simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3735851",
   "metadata": {},
   "source": [
    "# -------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa3daab",
   "metadata": {},
   "source": [
    "### Q6. Are ensemble techniques always better than individual models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbe2714",
   "metadata": {},
   "source": [
    "* Ensemble techniques are not always better than individual models. While ensemble techniques can improve the accuracy, robustness, and generalization performance of models, there are cases where they may not be effective.\n",
    "<br>\n",
    "\n",
    "* For example, if the individual models in the ensemble are very similar to each other, then the ensemble may not provide much benefit over a single model. Additionally, if the individual models are very complex and overfit the training data, then the ensemble may also overfit the data and not generalize well to new data.\n",
    "<br>\n",
    "\n",
    "* Furthermore, ensemble techniques can be computationally expensive and may require more resources to train and deploy compared to individual models.\n",
    "<br>\n",
    "\n",
    "* Therefore, it is important to carefully evaluate the effectiveness of ensemble techniques in a given application and compare their performance against individual models before deciding to use them. It is also important to consider the computational cost and feasibility of implementing ensemble techniques in the given application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b1d805",
   "metadata": {},
   "source": [
    "# -------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f369ea6",
   "metadata": {},
   "source": [
    "### Q7. How is the confidence interval calculated using bootstrap?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b8d8f5",
   "metadata": {},
   "source": [
    "* The confidence interval can be calculated using bootstrap as follows:\n",
    "\n",
    "    * Randomly select a sample of size n from the original data, with replacement. This is called a bootstrap sample.\n",
    "    * Calculate the statistic of interest (e.g., mean, median, standard deviation) for the bootstrap sample.\n",
    "    * Repeat steps 1 and 2 B times, where B is a large number (e.g., 1000).\n",
    "    * Calculate the standard error of the statistic by computing the standard deviation of the B bootstrap statistics.\n",
    "    * Calculate the lower and upper bounds of the confidence interval using the percentile method. For example, if we want to calculate a 95% confidence interval, we would take the 2.5th and 97.5th percentiles of the B bootstrap statistics. The resulting range represents the lower and upper bounds of the confidence interval.\n",
    "* The percentile method assumes that the distribution of the bootstrap statistics is approximately normal. If the distribution is not normal, other methods such as bias-corrected accelerated (BCA) bootstrap or studentized bootstrap can be used to calculate the confidence interval. Bootstrap is a powerful technique for estimating the uncertainty of a statistic, and it can be used in a wide range of applications, including hypothesis testing and model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c2a91",
   "metadata": {},
   "source": [
    "# -------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca39336",
   "metadata": {},
   "source": [
    "### Q8. How does bootstrap work and What are the steps involved in bootstrap?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91f2c18",
   "metadata": {},
   "source": [
    "* Bootstrap is a statistical technique used to estimate the variability and uncertainty of a population parameter by resampling the available data. It is particularly useful when the sample size is small or when the underlying distribution is not well known. The steps involved in bootstrap are as follows:\n",
    "\n",
    "    * Collect a sample of size n from the population of interest.\n",
    "    * Create a bootstrap sample by randomly selecting n observations from the original sample with replacement. This means that each observation in the original sample has an equal chance of being selected for the bootstrap sample, and some observations may be selected more than once.\n",
    "    * Calculate the statistic of interest (e.g., mean, standard deviation, correlation coefficient) for the bootstrap sample.\n",
    "    * Repeat steps 2 and 3 B times, where B is a large number (e.g., 1000). This will result in B bootstrap samples and B corresponding statistics of interest.\n",
    "    * Calculate the standard error of the statistic by taking the standard deviation of the B bootstrap statistics.\n",
    "    * Construct a confidence interval for the population parameter by using the percentile method. This involves selecting the α/2 and 1-α/2 percentiles of the B bootstrap statistics, where α is the desired level of significance (e.g., 0.05 for a 95% confidence interval).\n",
    "    * Interpret the confidence interval in the context of the problem.\n",
    "<br>\n",
    "\n",
    "* Bootstrap can be implemented using various statistical software packages such as R, Python, and SAS. It is a powerful and flexible technique that can be used for a wide range of statistical analyses, including hypothesis testing, parameter estimation, and model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26c9796",
   "metadata": {},
   "source": [
    "# -------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eb573a",
   "metadata": {},
   "source": [
    "### Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a8c02e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample mean height for 50 Trees is 15 and Sample Standard Deviation is 2\n",
      "T-Statistic with 95.0% condifence interval for dof 49 : 2.0096\n",
      "Standard Error : 0.2828\n",
      "Margin of error : 0.5684\n",
      "Estimated Population mean with 95% confidence interval is (14.43 , 15.57)\n"
     ]
    }
   ],
   "source": [
    "# Given Data \n",
    "samples = 50\n",
    "sample_mean = 15\n",
    "sample_std = 2\n",
    "confidence_level = 0.95\n",
    "\n",
    "# Calculate the t value for desired level of confidence\n",
    "import scipy.stats as stats\n",
    "alpha = 1 - confidence_level\n",
    "dof = samples-1\n",
    "t_value = stats.t.ppf(1 - alpha/2, dof)\n",
    "\n",
    "# calculate the standard error and margin of error\n",
    "import math\n",
    "std_error = sample_std / math.sqrt(samples)\n",
    "margin_of_error = t_value * std_error\n",
    "\n",
    "# calculate the confidence interval bounds\n",
    "lower_bound = sample_mean - margin_of_error\n",
    "upper_bound = sample_mean + margin_of_error\n",
    "\n",
    "# print 95% confidence interval\n",
    "print(f'Sample mean height for {samples} Trees is {sample_mean} and Sample Standard Deviation is {sample_std}')\n",
    "print(f'T-Statistic with {confidence_level*100}% condifence interval for dof {dof} : {t_value:.4f}')\n",
    "print(f'Standard Error : {std_error:.4f}')\n",
    "print(f'Margin of error : {margin_of_error:.4f}')\n",
    "print(f'Estimated Population mean with 95% confidence interval is ({lower_bound:.2f} , {upper_bound:.2f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a30615d",
   "metadata": {},
   "source": [
    "# -------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
